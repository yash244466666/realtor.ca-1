import {
  scrapeFromListingsPage,
  scrapeFromListingsPageWithPagination,
  scrapeFromListingsPageWithDynamicUpdates,
  scrapeFromListingsPageUltraMemoryEfficient,
  scrapeFromListingsPageWithStreaming,
  scrapeFromListingsPageWithUltraStreaming,
} from "./workflow";
import { saveToCSV, saveToJSON, generateTimestamp } from "./utils";
import { ScrapingConfig, ConfigPresets } from "./config";

async function main() {
  // ============ CONFIGURATION ============
  // You can easily change these settings or use one of the presets below

  // Option 1: Use current config from config.ts
  const config = ScrapingConfig;

  // Option 2: Use a preset (uncomment one of these if needed)
  // const config = ConfigPresets.PRODUCTION;   // Production settings

  // Option 3: Override specific settings (example)
  // const config = {
  //   ...ScrapingConfig,
  //   MAX_PAGES: 5,
  //   MAX_PROPERTIES: 50,
  //   PROPERTY_SCRAPING_DELAY: 2000
  // };

  const PAGE: string = config.DEFAULT_LISTING_URL;
  const ITEMS_TO_SCRAPE: number = config.MAX_PROPERTIES;
  const MAX_PAGES: number = config.MAX_PAGES;
  const USE_PAGINATION: boolean = config.USE_PAGINATION;
  const HEADLESS_MODE: boolean = config.HEADLESS_MODE;
  const USE_DYNAMIC_UPDATES: boolean = config.USE_DYNAMIC_UPDATES || true; // Default to true
  const MEMORY_MODE: string = config.MEMORY_MODE || "efficient";

  // Enhanced Realtor.ca Property Scraper with Configurable Settings
  // All timing and scraping parameters can be easily adjusted in config.ts
  // and saving results in both JSON and CSV formats

  // Display header with execution timestamp
  const startTime = new Date();
  console.log("\n=======================================");
  console.log("üè† Realtor.ca Property Scraper - Configurable Version");
  console.log(`‚è∞ Started at: ${startTime.toISOString()}`);
  console.log("=======================================");
  console.log(`‚öôÔ∏è  Configuration Summary:`);
  console.log(`   üéØ Max Properties: ${ITEMS_TO_SCRAPE}`);
  console.log(`   üìÑ Max Pages: ${MAX_PAGES}`);
  console.log(
    `   üîÑ Use Pagination: ${USE_PAGINATION ? "Enabled" : "Disabled"}`
  );
  console.log(
    `   üëÅÔ∏è  Headless Mode: ${HEADLESS_MODE ? "Enabled" : "Disabled"}`
  );
  console.log(`   üß† Memory Mode: ${MEMORY_MODE.toUpperCase()}`);
  console.log(
    `   ‚è±Ô∏è  Property Delay: ${config.PROPERTY_SCRAPING_DELAY / 1000}s`
  );
  console.log(`   üìç Target URL: ${PAGE}`);
  console.log("=======================================\n");

  try {
    let results;

    if (USE_DYNAMIC_UPDATES && USE_PAGINATION) {
      // Choose memory mode for dynamic updates
      if (MEMORY_MODE === "ultra-streaming") {
        // üöÄ ULTRA STREAMING: Real-time processing + Direct Excel export
        console.log("üöÄ Execution Mode: ULTRA STREAMING");
        console.log(
          "üìã Process: Real-time URL extraction ‚Üí Immediate property processing ‚Üí Direct Excel export"
        );
        console.log(
          "üìÅ Output: Direct Excel file streaming (daily + master files)"
        );
        console.log("üß† Memory Usage: MINIMAL (zero data accumulation)");
        console.log(
          "‚ö° Performance: MAXIMUM efficiency - no waiting for URL collection\n"
        );

        const ultraStreamResult =
          await scrapeFromListingsPageWithUltraStreaming(
            PAGE,
            HEADLESS_MODE,
            ITEMS_TO_SCRAPE,
            MAX_PAGES
          );

        const endTime = new Date();
        const duration = endTime.getTime() - startTime.getTime();

        console.log("\n=== ULTRA STREAMING EXECUTION SUMMARY ===");
        console.log(`‚úÖ Status: Completed successfully`);
        console.log(
          `üìä Properties Processed: ${ultraStreamResult.totalProcessed}`
        );
        console.log(`üìÅ Daily File: ${ultraStreamResult.dailyFile}`);
        console.log(`üìÅ Master File: ${ultraStreamResult.masterFile}`);
        console.log(`‚è∞ Start Time: ${startTime.toISOString()}`);
        console.log(`‚è∞ End Time: ${endTime.toISOString()}`);
        console.log(`‚è±Ô∏è  Total Duration: ${Math.round(duration / 1000)}s`);
        console.log("üéØ All data has been streamed directly to Excel files");
        return; // No results to save since everything was streamed
      } else if (MEMORY_MODE === "streaming") {
        // üöÄ STREAMING PIPELINE: Process URLs as they're discovered
        console.log("üöÄ Execution Mode: STREAMING PIPELINE");
        console.log(
          "üìã Process: Real-time URL extraction ‚Üí Immediate property processing"
        );
        console.log("üìÅ Output: Standard JSON/CSV files after completion");
        console.log(
          "üß† Memory Usage: Controlled (processes properties as URLs are found)"
        );
        console.log(
          "‚ö° Performance: HIGH efficiency - no waiting for full URL collection\n"
        );

        results = await scrapeFromListingsPageWithStreaming(
          PAGE,
          HEADLESS_MODE,
          ITEMS_TO_SCRAPE,
          MAX_PAGES
        );
      } else if (MEMORY_MODE === "ultra") {
        // üöÄ ULTRA MEMORY EFFICIENT: No data kept in memory
        console.log("üöÄ Execution Mode: ULTRA Memory-Efficient Scraping");
        console.log(
          "üìã Process: Real-time data streaming with zero memory accumulation"
        );
        console.log(
          "üìÅ Output: Direct Excel file updates (daily + master files)"
        );
        console.log("üß† Memory Usage: Minimal (data not stored in RAM)");
        console.log("‚ö° Performance: Optimized for large datasets\n");

        const ultraResult = await scrapeFromListingsPageUltraMemoryEfficient(
          PAGE,
          HEADLESS_MODE,
          ITEMS_TO_SCRAPE,
          MAX_PAGES
        );

        const endTime = new Date();
        const duration = endTime.getTime() - startTime.getTime();

        console.log("\n=== ULTRA MODE EXECUTION SUMMARY ===");
        console.log(`‚úÖ Status: Completed successfully`);
        console.log(`üìä Properties Processed: ${ultraResult.totalProcessed}`);
        console.log(`ÔøΩ Daily File: ${ultraResult.dailyFile}`);
        console.log(`üìÅ Master File: ${ultraResult.masterFile}`);
        console.log(`‚è∞ Start Time: ${startTime.toISOString()}`);
        console.log(`‚è∞ End Time: ${endTime.toISOString()}`);
        console.log(`‚è±Ô∏è  Total Duration: ${Math.round(duration / 1000)}s`);
        console.log("üéØ All data has been saved to Excel files");
        return; // No results to save since everything was streamed
      } else {
        // üöÄ EFFICIENT: Use temp files + limited memory
        console.log(
          "üöÄ Execution Mode: Memory-Efficient Scraping with Dynamic Updates"
        );
        console.log("üìã Process: Temp file streaming + dynamic Excel updates");
        console.log(
          "üìÅ Output: Daily/Master file system with real-time updates"
        );
        console.log(
          "üß† Memory Usage: Controlled (uses temporary file buffers)"
        );
        console.log("‚ö° Performance: Balanced memory usage and speed\n");

        results = await scrapeFromListingsPageWithDynamicUpdates(
          PAGE,
          HEADLESS_MODE,
          ITEMS_TO_SCRAPE,
          MAX_PAGES
        );
      }
    } else if (USE_PAGINATION) {
      // Standard pagination (keeps all data in memory)
      console.log("üöÄ Execution Mode: Standard Pagination Scraping");
      console.log(
        "üìã Process: Traditional pagination with full data retention"
      );
      console.log(
        "‚ö†Ô∏è  Memory Warning: All scraped data kept in RAM until completion"
      );
      console.log("üß† Memory Usage: High (entire dataset stored in memory)");
      console.log("‚ö° Performance: Fast but memory-intensive\n");

      results = await scrapeFromListingsPageWithPagination(
        PAGE,
        HEADLESS_MODE,
        ITEMS_TO_SCRAPE,
        MAX_PAGES
      );
    } else {
      // Single page scraping (original method)
      console.log("üöÄ Execution Mode: Single Page Scraping");
      console.log("üìã Process: Limited to first page only (legacy mode)");
      console.log("üß† Memory Usage: Low (single page data only)");
      console.log("‚ö° Performance: Fast but limited scope\n");

      results = await scrapeFromListingsPage(
        PAGE,
        HEADLESS_MODE,
        ITEMS_TO_SCRAPE
      );
    }

    if (results.length > 0) {
      // Calculate execution metrics
      const endTime = new Date();
      const duration = endTime.getTime() - startTime.getTime();
      const propertiesPerSecond = (results.length / (duration / 1000)).toFixed(
        2
      );

      // Save results with timestamp
      const timestamp = generateTimestamp();
      const jsonFilename = `listings-scrape-${timestamp}.json`;
      const csvFilename = `listings-scrape-${timestamp}.csv`;

      saveToJSON(results, jsonFilename);
      saveToCSV(results, csvFilename);

      console.log("\n=== EXECUTION SUMMARY ===");
      console.log(`‚úÖ Status: Scraping completed successfully`);
      console.log(`üìä Properties Scraped: ${results.length}`);
      console.log(`‚è∞ Start Time: ${startTime.toISOString()}`);
      console.log(`‚è∞ End Time: ${endTime.toISOString()}`);
      console.log(`‚è±Ô∏è  Total Duration: ${Math.round(duration / 1000)}s`);
      console.log(`‚ö° Average Speed: ${propertiesPerSecond} properties/second`);
      console.log(`üíæ JSON Output: ${jsonFilename}`);
      console.log(`üíæ CSV Output: ${csvFilename}`);

      console.log("\n=== SAMPLE DATA PREVIEW ===");
      console.table(results.slice(0, 3)); // Show first 3 properties as preview
    } else {
      console.log("\n=== EXECUTION SUMMARY ===");
      console.log("‚ùå Status: No properties were scraped");
      console.log(
        "üí° Suggestion: Check if the target URL is valid and contains listings"
      );
    }
  } catch (error) {
    const endTime = new Date();
    const duration = endTime.getTime() - startTime.getTime();

    console.log("\n=== EXECUTION FAILED ===");
    console.log("‚ùå Status: Scraping failed due to error");
    console.log(`‚è∞ Failed at: ${endTime.toISOString()}`);
    console.log(`‚è±Ô∏è  Runtime before failure: ${Math.round(duration / 1000)}s`);
    console.error("üêõ Error Details:", error);
    console.log(
      "üí° Suggestion: Check network connection, target URL, or configuration settings"
    );
  }

  return;
}

// Run the scraper
main();
