import { RealtorCaScraper, PropertyData } from "./scraper";
import { ScrapingConfig, TimingConfig } from "./config";
import {
  initializeDynamicExcel,
  addPropertyToExcel,
  finalizeDynamicExcel,
  addPropertyToMemoryEfficientSystem,
  loadAllDataFromTempFiles,
  cleanupTempFiles,
  getMemoryStats,
} from "./utils";

// Type for the streaming callback function
type PropertyStreamCallback = (
  property: PropertyData,
  index: number,
  total?: number
) => Promise<void>;

// Function to scrape from listings page and then scrape each property
export async function scrapeFromListingsPage(
  listingPageUrl: string = ScrapingConfig.DEFAULT_LISTING_URL,
  headless: boolean = ScrapingConfig.HEADLESS_MODE,
  maxProperties: number = ScrapingConfig.DEFAULT_SINGLE_PAGE_LIMIT
): Promise<PropertyData[]> {
  const scraper = new RealtorCaScraper();
  const results: PropertyData[] = [];

  try {
    await scraper.initialize();

    // First, extract URLs from the listings page using API
    console.log("üîç Step 1: API URL Extraction Process");
    console.log("üì° Connecting to realtor.ca API to extract property URLs...");
    const urlExtractionStart = Date.now();

    const propertyUrls = await scraper.extractPropertyUrls(1); // Just 1 page for single page scraping

    const urlExtractionTime = Date.now() - urlExtractionStart;
    console.log(
      `‚úÖ URL Extraction completed in ${Math.round(urlExtractionTime / 1000)}s`
    );
    console.log(`üìä Found ${propertyUrls.length} property URLs`);

    if (propertyUrls.length === 0) {
      console.log("‚ùå No property URLs found on the listings page");
      console.log(
        "üí° This could indicate: Invalid URL, no listings, or network issues"
      );
      return results;
    }

    // Limit the number of properties to scrape
    const urlsToScrape = propertyUrls.slice(0, maxProperties);
    console.log(`\nÔøΩ Step 2: Property Data Scraping Process`);
    console.log(
      `üìã Processing ${urlsToScrape.length} properties (limited from ${propertyUrls.length} available)`
    );
    console.log(
      `‚è±Ô∏è  Estimated time: ~${Math.round(
        (urlsToScrape.length * ScrapingConfig.PROPERTY_SCRAPING_DELAY) /
          1000 /
          60
      )} minutes`
    );

    // Scrape each property
    const scrapingStartTime = Date.now();
    for (let i = 0; i < urlsToScrape.length; i++) {
      const currentTime = new Date().toISOString().split("T")[1].split(".")[0];
      console.log(
        `\n[${currentTime}] üìç Processing ${i + 1}/${urlsToScrape.length}: ${
          urlsToScrape[i]
        }`
      );

      try {
        const propertyStart = Date.now();
        const propertyData = await scraper.scrapeProperty(urlsToScrape[i]);
        const propertyTime = Date.now() - propertyStart;

        results.push(propertyData);

        // Log the individual property data with timing
        console.log(
          `‚úÖ Property ${i + 1} scraped successfully in ${Math.round(
            propertyTime / 1000
          )}s`
        );
        console.log(`üìä Address: ${propertyData.ADDRESS || "N/A"}`);
        console.log(`üí∞ Price: ${propertyData.PRICE || "N/A"}`);
        console.log(`ÔøΩÔ∏è City: ${propertyData.CITY || "N/A"}`);
        console.log(
          `üìç Location: ${
            propertyData.LATITUDE && propertyData.LONGITUDE
              ? `${propertyData.LATITUDE}, ${propertyData.LONGITUDE}`
              : "N/A"
          }`
        );

        const remaining = urlsToScrape.length - (i + 1);
        if (remaining > 0) {
          const avgTimePerProperty = (Date.now() - scrapingStartTime) / (i + 1);
          const estimatedTimeLeft = Math.round(
            (remaining * avgTimePerProperty) / 1000 / 60
          );
          console.log(
            `‚è≥ Estimated time remaining: ${estimatedTimeLeft} minutes`
          );
        }

        // Add delay between requests to be respectful
        if (i < urlsToScrape.length - 1) {
          console.log(
            `‚è≥ Waiting ${
              TimingConfig.PROPERTY_SCRAPING_DELAY / 1000
            } seconds before next request...`
          );
          await new Promise((resolve) =>
            setTimeout(resolve, TimingConfig.PROPERTY_SCRAPING_DELAY)
          );
        }
      } catch (error) {
        console.error(`Failed to scrape ${urlsToScrape[i]}:`, error);
        // Add a placeholder for failed scrapes
        results.push({
          DATE: new Date().toLocaleDateString("en-GB"),
          ADDRESS: "Error",
          CITY: "Error",
          STATE: "Error",
          POSTAL: "Error",
          AGENT: "Error",
          BROKER: "Error",
          PRICE: "Error",
          LATITUDE: "Error",
          LONGITUDE: "Error",
        });
      }
    }

    console.log(`\n‚úÖ Completed scraping ${results.length} properties`);
  } finally {
    await scraper.close();
  }

  return results;
}

// Function to scrape from listings page with pagination support
export async function scrapeFromListingsPageWithPagination(
  listingPageUrl: string = ScrapingConfig.DEFAULT_LISTING_URL,
  headless: boolean = ScrapingConfig.HEADLESS_MODE,
  maxProperties: number = ScrapingConfig.MAX_PROPERTIES,
  maxPages: number = ScrapingConfig.MAX_PAGES
): Promise<PropertyData[]> {
  const scraper = new RealtorCaScraper();
  const results: PropertyData[] = [];

  try {
    await scraper.initialize();

    // Extract URLs from multiple pages using the API approach
    console.log("üîç Step 1: Extracting property URLs with pagination...");
    const propertyUrls = await scraper.extractPropertyUrls(maxPages);

    if (propertyUrls.length === 0) {
      console.log("‚ùå No property URLs found on the listings pages");
      return results;
    }

    // Limit the number of properties to scrape
    const urlsToScrape = propertyUrls.slice(0, maxProperties);
    console.log(
      `üìã Step 2: Scraping ${urlsToScrape.length} properties from ${propertyUrls.length} total found...`
    );

    // Scrape each property
    for (let i = 0; i < urlsToScrape.length; i++) {
      console.log(
        `\nüìç Processing ${i + 1}/${urlsToScrape.length}: ${urlsToScrape[i]}`
      );
      try {
        const propertyData = await scraper.scrapeProperty(urlsToScrape[i]);
        results.push(propertyData);

        // Log the individual property data
        console.log(`\n=== PROPERTY ${i + 1} DATA ===`);
        console.table(propertyData);

        // Add delay between requests to be respectful
        if (i < urlsToScrape.length - 1) {
          console.log(
            `‚è≥ Waiting ${
              TimingConfig.PROPERTY_SCRAPING_DELAY / 1000
            } seconds before next request...`
          );
          await new Promise((resolve) =>
            setTimeout(resolve, TimingConfig.PROPERTY_SCRAPING_DELAY)
          );
        }
      } catch (error) {
        console.error(`Failed to scrape ${urlsToScrape[i]}:`, error);
        // Add a placeholder for failed scrapes
        results.push({
          DATE: new Date().toLocaleDateString("en-GB"),
          ADDRESS: "Error",
          CITY: "Error",
          STATE: "Error",
          POSTAL: "Error",
          AGENT: "Error",
          BROKER: "Error",
          PRICE: "Error",
          LATITUDE: "Error",
          LONGITUDE: "Error",
        });
      }
    }

    console.log(
      `\n‚úÖ Completed scraping ${results.length} properties with pagination`
    );
  } finally {
    await scraper.close();
  }

  return results;
}

// Memory-efficient function with dynamic Excel updates and temp file management
export async function scrapeFromListingsPageWithDynamicUpdates(
  listingPageUrl: string = ScrapingConfig.DEFAULT_LISTING_URL,
  headless: boolean = ScrapingConfig.HEADLESS_MODE,
  maxProperties: number = ScrapingConfig.MAX_PROPERTIES,
  maxPages: number = ScrapingConfig.MAX_PAGES
): Promise<PropertyData[]> {
  const scraper = new RealtorCaScraper();
  let propertiesProcessed = 0;

  try {
    await scraper.initialize();

    // Initialize dynamic Excel files (daily + master)
    await initializeDynamicExcel();
    console.log("üìä Dynamic Excel files initialized");
    console.log(
      "üß† Memory-efficient system activated - data will be streamed to temp files"
    );

    // Extract URLs with pagination
    console.log("üîç Step 1: Extracting property URLs with pagination...");
    const propertyUrls = await scraper.extractPropertyUrls(maxPages);

    if (propertyUrls.length === 0) {
      console.log("‚ùå No property URLs found on the listings pages");
      return [];
    }

    // Limit the number of properties to scrape
    const urlsToScrape = propertyUrls.slice(0, maxProperties);
    console.log(
      `üìã Step 2: Scraping ${urlsToScrape.length} properties from ${propertyUrls.length} total found...`
    );
    console.log(
      "üíæ Each property will be saved immediately to Excel + temp files for memory efficiency"
    );

    // Scrape each property with memory-efficient system
    for (let i = 0; i < urlsToScrape.length; i++) {
      console.log(
        `\nüìç Processing ${i + 1}/${urlsToScrape.length}: ${urlsToScrape[i]}`
      );

      try {
        const propertyData = await scraper.scrapeProperty(urlsToScrape[i]);

        // üöÄ MEMORY-EFFICIENT UPDATE: Add to temp file system
        await addPropertyToMemoryEfficientSystem(propertyData);
        propertiesProcessed++;

        // Log the individual property data
        console.log(`\n=== PROPERTY ${i + 1} DATA ===`);
        console.table(propertyData);

        // Show memory stats every 10 properties
        if (propertiesProcessed % 10 === 0) {
          const memStats = getMemoryStats();
          console.log(
            `\nüß† Memory Stats: ${memStats.propertiesInMemory} in RAM, ${memStats.tempFilesCount} temp files, ~${memStats.estimatedMemoryMB}MB used`
          );
        }

        // Add delay between requests to be respectful
        if (i < urlsToScrape.length - 1) {
          console.log(
            `‚è≥ Waiting ${
              TimingConfig.PROPERTY_SCRAPING_DELAY / 1000
            } seconds before next request...`
          );
          await new Promise((resolve) =>
            setTimeout(resolve, TimingConfig.PROPERTY_SCRAPING_DELAY)
          );
        }
      } catch (error) {
        console.error(`Failed to scrape ${urlsToScrape[i]}:`, error);
        // Add a placeholder for failed scrapes (but don't store in memory array)
        const errorProperty: PropertyData = {
          DATE: new Date().toLocaleDateString("en-GB"),
          ADDRESS: `Error-${i}`,
          CITY: "Error",
          STATE: "Error",
          POSTAL: "Error",
          AGENT: "Error",
          BROKER: "Error",
          PRICE: "Error",
          LATITUDE: "Error",
          LONGITUDE: "Error",
        };
        await addPropertyToMemoryEfficientSystem(errorProperty);
        propertiesProcessed++;
      }
    }

    console.log(
      `\n‚úÖ Completed scraping ${propertiesProcessed} properties with memory-efficient system`
    );

    // Finalize Excel files
    const { dailyFile, masterFile } = await finalizeDynamicExcel();
    console.log(`\nüìä Files created/updated:`);
    console.log(`   üìÖ Daily file: ${dailyFile}`);
    console.log(`   üìö Master file: ${masterFile}`);

    // Load all data from temp files for final return (if needed)
    console.log(
      `\nüìñ Loading all data from temp files for final processing...`
    );
    const allData = await loadAllDataFromTempFiles();

    // Cleanup temp files
    await cleanupTempFiles();

    return allData;
  } finally {
    await scraper.close();
  }
}

// Ultra memory-efficient function for large-scale scraping (no return array)
export async function scrapeFromListingsPageUltraMemoryEfficient(
  listingPageUrl: string = ScrapingConfig.DEFAULT_LISTING_URL,
  headless: boolean = ScrapingConfig.HEADLESS_MODE,
  maxProperties: number = ScrapingConfig.MAX_PROPERTIES,
  maxPages: number = ScrapingConfig.MAX_PAGES
): Promise<{ totalProcessed: number; dailyFile: string; masterFile: string }> {
  const scraper = new RealtorCaScraper();
  let propertiesProcessed = 0;

  try {
    await scraper.initialize();

    // Initialize dynamic Excel files (daily + master)
    await initializeDynamicExcel();
    console.log("üìä Dynamic Excel files initialized");
    console.log(
      "üöÄ ULTRA MEMORY-EFFICIENT MODE: No data kept in memory - streaming directly to files"
    );

    // Extract URLs with pagination
    console.log("üîç Step 1: Extracting property URLs with pagination...");
    const propertyUrls = await scraper.extractPropertyUrls(maxPages);

    if (propertyUrls.length === 0) {
      console.log("‚ùå No property URLs found on the listings pages");
      return { totalProcessed: 0, dailyFile: "", masterFile: "" };
    }

    // Limit the number of properties to scrape
    const urlsToScrape = propertyUrls.slice(0, maxProperties);
    console.log(
      `üìã Step 2: Scraping ${urlsToScrape.length} properties from ${propertyUrls.length} total found...`
    );
    console.log(
      "üíæ Each property streams directly to Excel files - ZERO memory accumulation"
    );

    // Scrape each property with direct streaming (no memory storage)
    for (let i = 0; i < urlsToScrape.length; i++) {
      console.log(
        `\nüìç Processing ${i + 1}/${urlsToScrape.length}: ${urlsToScrape[i]}`
      );

      try {
        const propertyData = await scraper.scrapeProperty(urlsToScrape[i]);

        // üöÄ DIRECT STREAM: Add directly to Excel files only (no memory storage)
        await addPropertyToExcel(propertyData);
        propertiesProcessed++;

        // Log the individual property data
        console.log(`\n=== PROPERTY ${i + 1} DATA ===`);
        console.table(propertyData);
        console.log(
          `üöÄ Streamed directly to Excel files - zero memory footprint`
        );

        // Show progress every 25 properties
        if (propertiesProcessed % 25 === 0) {
          console.log(
            `\nüìä Progress: ${propertiesProcessed} properties processed - Memory usage: MINIMAL`
          );
        }

        // Add delay between requests to be respectful
        if (i < urlsToScrape.length - 1) {
          console.log(
            `‚è≥ Waiting ${
              TimingConfig.PROPERTY_SCRAPING_DELAY / 1000
            } seconds before next request...`
          );
          await new Promise((resolve) =>
            setTimeout(resolve, TimingConfig.PROPERTY_SCRAPING_DELAY)
          );
        }
      } catch (error) {
        console.error(`Failed to scrape ${urlsToScrape[i]}:`, error);
        // Add a placeholder for failed scrapes (directly to Excel, no memory)
        const errorProperty: PropertyData = {
          DATE: new Date().toLocaleDateString("en-GB"),
          ADDRESS: `Error-${i}`,
          CITY: "Error",
          STATE: "Error",
          POSTAL: "Error",
          AGENT: "Error",
          BROKER: "Error",
          PRICE: "Error",
          LATITUDE: "Error",
          LONGITUDE: "Error",
        };
        await addPropertyToExcel(errorProperty);
        propertiesProcessed++;
      }
    }

    console.log(
      `\n‚úÖ Completed scraping ${propertiesProcessed} properties with ULTRA memory-efficient streaming`
    );

    // Finalize Excel files
    const { dailyFile, masterFile } = await finalizeDynamicExcel();
    console.log(`\nüìä Files created/updated:`);
    console.log(`   üìÖ Daily file: ${dailyFile}`);
    console.log(`   üìö Master file: ${masterFile}`);
    console.log(
      `\nüß† Memory efficiency: NO data stored in memory - everything streamed directly to files`
    );

    return {
      totalProcessed: propertiesProcessed,
      dailyFile,
      masterFile,
    };
  } finally {
    await scraper.close();
  }
}

/**
 * üöÄ STREAMING PIPELINE: Extract URLs and scrape properties simultaneously
 * This approach processes properties as soon as their URLs are extracted,
 * significantly improving performance by parallelizing operations.
 */
export async function scrapeFromListingsPageWithStreaming(
  listingPageUrl: string = ScrapingConfig.DEFAULT_LISTING_URL,
  headless: boolean = ScrapingConfig.HEADLESS_MODE,
  maxProperties: number = ScrapingConfig.MAX_PROPERTIES,
  maxPages: number = ScrapingConfig.MAX_PAGES,
  onPropertyScraped?: PropertyStreamCallback
): Promise<PropertyData[]> {
  const scraper = new RealtorCaScraper();
  const results: PropertyData[] = [];
  let processedCount = 0;
  const startTime = Date.now();

  try {
    await scraper.initialize();

    console.log("üöÄ STREAMING PIPELINE MODE");
    console.log("=========================");
    console.log("‚ö° Processing properties as URLs are discovered");
    console.log(
      `üéØ Target: ${maxProperties} properties from ${maxPages} pages`
    );
    console.log(`üìç Source: ${listingPageUrl}`);
    console.log("=========================\n");

    // Start streaming URL extraction and property scraping
    await scraper.streamPropertyScraping(
      maxPages,
      maxProperties,
      async (propertyUrl: string, urlIndex: number, estimatedTotal: number) => {
        if (processedCount >= maxProperties) {
          return false; // Stop processing
        }

        const propertyStart = Date.now();
        const currentTime = new Date()
          .toISOString()
          .split("T")[1]
          .split(".")[0];

        console.log(
          `\n[${currentTime}] üè† Processing Property ${
            processedCount + 1
          }/${maxProperties}`
        );
        console.log(`üîó URL: ${propertyUrl}`);
        console.log(
          `üìä Progress: ${(
            ((processedCount + 1) / maxProperties) *
            100
          ).toFixed(1)}%`
        );

        try {
          // Scrape the property data
          const propertyData = await scraper.scrapeProperty(propertyUrl);
          const propertyTime = Date.now() - propertyStart;

          // Add to results
          results.push(propertyData);
          processedCount++;

          // Log success with details
          console.log(
            `‚úÖ Property scraped in ${Math.round(propertyTime / 1000)}s`
          );
          console.log(`   üìç Address: ${propertyData.ADDRESS || "N/A"}`);
          console.log(`   üí∞ Price: ${propertyData.PRICE || "N/A"}`);
          console.log(`   üèôÔ∏è  City: ${propertyData.CITY || "N/A"}`);

          // Calculate performance metrics
          const elapsed = Date.now() - startTime;
          const avgTimePerProperty = elapsed / processedCount;
          const remaining = maxProperties - processedCount;
          const estimatedTimeLeft = Math.round(
            (remaining * avgTimePerProperty) / 1000 / 60
          );

          if (remaining > 0) {
            console.log(
              `‚è≥ Estimated time remaining: ${estimatedTimeLeft} minutes`
            );
            console.log(
              `‚ö° Average speed: ${(
                (processedCount / (elapsed / 1000)) *
                60
              ).toFixed(1)} properties/hour`
            );
          }

          // Call custom callback if provided
          if (onPropertyScraped) {
            await onPropertyScraped(
              propertyData,
              processedCount,
              maxProperties
            );
          }

          // Rate limiting delay
          if (processedCount < maxProperties) {
            console.log(
              `‚è≥ Rate limiting delay: ${
                TimingConfig.PROPERTY_SCRAPING_DELAY / 1000
              }s`
            );
            await new Promise((resolve) =>
              setTimeout(resolve, TimingConfig.PROPERTY_SCRAPING_DELAY)
            );
          }

          return true; // Continue processing
        } catch (error) {
          console.error(
            `‚ùå Failed to scrape property ${processedCount + 1}:`,
            error
          );
          console.log(`üîó Problem URL: ${propertyUrl}`);
          console.log(`üí° Continuing with next property...`);

          // Add error placeholder
          results.push({
            DATE: new Date().toLocaleDateString("en-GB"),
            ADDRESS: `SCRAPING_ERROR_${processedCount + 1}`,
            CITY: "Error",
            STATE: "Error",
            POSTAL: "Error",
            AGENT: "Error",
            BROKER: "Error",
            PRICE: "Error",
            LATITUDE: "Error",
            LONGITUDE: "Error",
          });
          processedCount++;

          return true; // Continue despite error
        }
      }
    );

    const totalTime = Date.now() - startTime;
    console.log("\nüéâ STREAMING PIPELINE COMPLETED");
    console.log("===============================");
    console.log(
      `‚úÖ Status: Successfully processed ${processedCount} properties`
    );
    console.log(`‚è±Ô∏è  Total time: ${Math.round(totalTime / 1000)}s`);
    console.log(
      `‚ö° Average speed: ${(processedCount / (totalTime / 1000)).toFixed(
        2
      )} properties/second`
    );
    console.log(`üíæ Results ready for export`);

    return results;
  } catch (error) {
    console.error("‚ùå Streaming pipeline error:", error);
    throw error;
  } finally {
    await scraper.close();
  }
}

/**
 * üöÄ ULTRA STREAMING: Memory-efficient streaming with direct Excel export
 * Combines URL streaming with direct Excel writing for maximum efficiency
 */
export async function scrapeFromListingsPageWithUltraStreaming(
  listingPageUrl: string = ScrapingConfig.DEFAULT_LISTING_URL,
  headless: boolean = ScrapingConfig.HEADLESS_MODE,
  maxProperties: number = ScrapingConfig.MAX_PROPERTIES,
  maxPages: number = ScrapingConfig.MAX_PAGES
): Promise<{ totalProcessed: number; dailyFile: string; masterFile: string }> {
  const scraper = new RealtorCaScraper();
  let processedCount = 0;
  const startTime = Date.now();

  try {
    await scraper.initialize();

    // Initialize Excel files for streaming
    const { dailyFile, masterFile } = await initializeDynamicExcel();

    console.log("üöÄ ULTRA STREAMING MODE");
    console.log("=======================");
    console.log("‚ö° Real-time property processing with direct Excel export");
    console.log("üß† Zero memory accumulation - immediate file streaming");
    console.log(
      `üéØ Target: ${maxProperties} properties from ${maxPages} pages`
    );
    console.log(`üìÅ Output: ${dailyFile} | ${masterFile}`);
    console.log("=======================\n");

    // Start ultra streaming
    await scraper.streamPropertyScraping(
      maxPages,
      maxProperties,
      async (propertyUrl: string, urlIndex: number, estimatedTotal: number) => {
        if (processedCount >= maxProperties) {
          return false;
        }

        const propertyStart = Date.now();
        const currentTime = new Date()
          .toISOString()
          .split("T")[1]
          .split(".")[0];

        console.log(
          `[${currentTime}] ‚ö° Streaming Property ${
            processedCount + 1
          }/${maxProperties}`
        );
        console.log(`üîó ${propertyUrl}`);

        try {
          // Scrape property data
          const propertyData = await scraper.scrapeProperty(propertyUrl);
          const propertyTime = Date.now() - propertyStart;

          // Immediately stream to Excel files
          await addPropertyToExcel(propertyData);
          processedCount++;

          console.log(
            `‚úÖ Streamed to Excel in ${Math.round(propertyTime / 1000)}s`
          );
          console.log(
            `üìä ${propertyData.ADDRESS || "N/A"} | ${
              propertyData.PRICE || "N/A"
            }`
          );

          // Show progress
          const elapsed = Date.now() - startTime;
          const rate = ((processedCount / (elapsed / 1000)) * 60).toFixed(1);
          console.log(
            `‚ö° Rate: ${rate} properties/hour | Progress: ${(
              (processedCount / maxProperties) *
              100
            ).toFixed(1)}%`
          );

          return true;
        } catch (error) {
          console.error(
            `‚ùå Error streaming property ${processedCount + 1}:`,
            error
          );

          // Stream error record to Excel
          const errorRecord = {
            DATE: new Date().toLocaleDateString("en-GB"),
            ADDRESS: `STREAMING_ERROR_${processedCount + 1}`,
            CITY: "Error",
            STATE: "Error",
            POSTAL: "Error",
            AGENT: "Error",
            BROKER: "Error",
            PRICE: "Error",
            LATITUDE: "Error",
            LONGITUDE: "Error",
          };

          await addPropertyToExcel(errorRecord);
          processedCount++;

          return true;
        }
      }
    );

    // Finalize Excel files
    await finalizeDynamicExcel();

    const totalTime = Date.now() - startTime;
    console.log("\nüéâ ULTRA STREAMING COMPLETED");
    console.log("============================");
    console.log(`‚úÖ Processed: ${processedCount} properties`);
    console.log(`‚è±Ô∏è  Duration: ${Math.round(totalTime / 1000)}s`);
    console.log(`üìÅ Files: ${dailyFile} | ${masterFile}`);
    console.log(`üß† Memory used: MINIMAL (no data accumulation)`);

    return {
      totalProcessed: processedCount,
      dailyFile,
      masterFile,
    };
  } catch (error) {
    console.error("‚ùå Ultra streaming error:", error);
    throw error;
  } finally {
    await scraper.close();
  }
}
